{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp9LGETOHtwB"
      },
      "source": [
        "# ICPR 2024 Competition on Multilingual Claim-Span Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNJLpOPTHyHm"
      },
      "source": [
        "## Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9us7olzx4Hw"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow\n",
        "#!pip install keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxTIjxhZIHq-"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XO89wVUrHl7C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, GRU,SimpleRNN\n",
        "from keras.layers import Dense, Activation, Dropout, Embedding, BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.metrics import jaccard_score, f1_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from plotly import graph_objs as go\n",
        "import plotly.express as px\n",
        "import plotly.figure_factory as ff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_YV527n2X-O"
      },
      "source": [
        "### Configuring TPU's\n",
        "\n",
        "For this version of Notebook we will be using TPU's as we have to built a BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0ua8I5A30jt"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "try:\n",
        "    # Detect TPU and create TPU cluster resolver\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "\n",
        "    # Connect to the TPU cluster\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "\n",
        "    # Initialize the TPU system\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "\n",
        "    # Create a TPUStrategy\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except (ValueError, tf.errors.NotFoundError):\n",
        "    tpu = None\n",
        "    # Default distribution strategy for CPU/GPU\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqtY2Y0kIRaU"
      },
      "source": [
        "### Loading Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwIgBITKH-j_"
      },
      "outputs": [],
      "source": [
        "# Training English and Hindi datasets\n",
        "train_data = pd.read_json(\"/content/drive/MyDrive/Multilingual Datasets/ML Data/train_en_hi_encoded_labels.json\")\n",
        "\n",
        "\n",
        "# Validation English and Hindi datasets\n",
        "val_data = pd.read_json(\"/content/drive/MyDrive/Multilingual Datasets/ML Data/val_en_hi_encoded_labels.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzahbuLT6kDM"
      },
      "outputs": [],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0kOdcspIyLt"
      },
      "outputs": [],
      "source": [
        "train_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srpXT-YSJOpd"
      },
      "outputs": [],
      "source": [
        "val_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME5MieUM66Nj"
      },
      "source": [
        "We will check the maximum number of words that can be present in a comment , this will help us in padding later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_IcXvC54rPk"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.loc[:12000,:]\n",
        "train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SM6nPxIj7Eyz"
      },
      "outputs": [],
      "source": [
        "train_data['text_tokens'].apply(lambda x:len(str(x).split())).max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhaEotmjFMq_"
      },
      "outputs": [],
      "source": [
        "X_val = val_data[\"text_tokens\"]\n",
        "y_val = val_data[\"claims\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlD7RVC-7uhE"
      },
      "source": [
        "Writing a function for getting auccuracy score for validation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "\n",
        "def roc_auc(predictions, target):\n",
        "    '''\n",
        "    This method returns the AUC Score and plots the ROC Curve when given the Predictions\n",
        "    and Labels\n",
        "    '''\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    # Plotting the ROC Curve\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC Score = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic(ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    return roc_auc"
      ],
      "metadata": {
        "id": "Hisb_6uPxp4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynrLD70N7_G7"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baAgkCjk74ve"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train_data.text_tokens.values, train_data.claims.values,\n",
        "                                                  stratify=train_data.claims.values,\n",
        "                                                  random_state=42,\n",
        "                                                  test_size=0.10, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training & Development"
      ],
      "metadata": {
        "id": "-CK7oHY5vjz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import text, sequence\n",
        "\n",
        "\n",
        "# Initialize the tokenizer\n",
        "token = text.Tokenizer(num_words=None)\n",
        "max_len = 1500\n",
        "\n",
        "# Fit tokenizer on training data only\n",
        "token.fit_on_texts(X_train)\n",
        "\n",
        "# Convert texts to sequences\n",
        "X_train_seq = token.texts_to_sequences(X_train)\n",
        "X_test_seq = token.texts_to_sequences(X_test)\n",
        "X_val_seq = token.texts_to_sequences(X_val)\n",
        "\n",
        "# Zero pad the sequences\n",
        "X_train_pad = sequence.pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_test_pad = sequence.pad_sequences(X_test_seq, maxlen=max_len)\n",
        "X_val_pad = sequence.pad_sequences(X_val_seq, maxlen=max_len)\n",
        "\n",
        "# Get the word index\n",
        "word_index = token.word_index"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZyNIQTMAFYtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YELpDilxTjD"
      },
      "source": [
        "### Word Embeddings\n",
        "\n",
        "The latest approach to getting word Embeddings is using pretained GLoVe or using Fasttext. Without going into too much details, I would explain how to create sentence vectors and how can we use them to create a machine learning model on top of it and since I am a fan of GloVe vectors, word2vec and fasttext. In this Notebook, I'll be using the GloVe vectors. You can download the GloVe vectors you can search for GloVe in datasets on Kaggle and add the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktnQYPTmjGkE"
      },
      "outputs": [],
      "source": [
        "# load the GloVe vectors in a dictionary:\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open('/content/drive/MyDrive/Multilingual Datasets/ GLoVe/glove.6B.300d.txt','r',encoding='utf-8')\n",
        "for line in tqdm(f):\n",
        "    values = line.split(' ')\n",
        "    word = values[0]\n",
        "    coefs = np.asarray([float(val) for val in values[1:]])\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl0auKueyAXS"
      },
      "source": [
        "We have already tokenized and paded our text for input to LSTM's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4xEHRLUx0hT"
      },
      "outputs": [],
      "source": [
        "# create an embedding matrix for the words we have in the dataset\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "for word, i in tqdm(word_index.items()):\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bi-Directional LSTM"
      ],
      "metadata": {
        "id": "8-1YT8C21S9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "    # A simple bidirectional LSTM with glove embeddings and one dense layer\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                     300,\n",
        "                     weights=[embedding_matrix],\n",
        "                     input_length=max_len,\n",
        "                     trainable=False))\n",
        "    model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
        "\n",
        "    model.add(Dense(1,activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Hj89Q6bD0m5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_pad, y_train, epochs=5, batch_size=64*strategy.num_replicas_in_sync)"
      ],
      "metadata": {
        "id": "7zMOhQXz16kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.predict(X_val_pad)"
      ],
      "metadata": {
        "id": "redXvy1f1-bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_model = []\n",
        "\n",
        "scores_model.append({'Model': 'Bi-directional LSTM','ROC Curve and AUC_Score': roc_auc(scores, y_val)})\n",
        "print(scores_model)"
      ],
      "metadata": {
        "id": "EcLo96_t2Dj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "# Step 1: Make predictions on the validation set\n",
        "predictions = (model.predict(X_val_pad) > 0.5).astype(int)\n",
        "\n",
        "# Step 2: Calculate Jaccard similarity score\n",
        "jaccard = jaccard_score(y_val, predictions)\n",
        "\n",
        "# Step 3: Calculate Macro-F1 score\n",
        "macro_f1 = f1_score(y_val, predictions, average='macro')\n",
        "\n",
        "print(\"Jaccard Similarity Score of Bi-Directional LSTM:\", jaccard)\n",
        "print(\"Macro-F1 Score of Bi-Directional LSTM:\", macro_f1)"
      ],
      "metadata": {
        "id": "oQJ_e8UKB4bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_model"
      ],
      "metadata": {
        "id": "cfwTNdFrGYYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QGOX-R_Z2k_z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}